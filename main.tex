\documentclass[parskip=half]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{authblk}
\PassOptionsToPackage{hyphens}{url}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\lstset{basicstyle=\ttfamily}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}

% Cross referencing external documents.
\usepackage{xr}
% Command for registering file dependencies with latexmk.
\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\externaldocument[s-]{supplement}
\addFileDependency{supplement.tex}
\addFileDependency{supplement.aux}

\usepackage[maxbibnames=999,backend=bibtex]{biblatex}
\addbibresource{literature.bib}

\newcommand{\sharedfirst}{$\dagger$}
\newcommand{\sharedfirsttext}[1]{\affil[\sharedfirst]{#1}}
\newcommand{\corresponding}{*}
\newcommand{\correspondingtext}[1]{\affil[\corresponding]{#1}}
\newcommand{\image}[1]{\centering\includegraphics[width=\textwidth]{#1}}
\let\plainurl\url
\renewcommand{\url}[1]{\protect\plainurl{#1}}

\begin{document}

\author[1,2]{Felix Mölder}
\author[6,15]{Kim Philipp Jablonski}
\author[4]{Michael B. Hall}
\author[4]{Brice Letcher}
\author[5]{Vanessa Sochat}
\author[3]{Soohyun Lee}
\author[8]{Sven~O.~Twardziok}
\author[9]{Alexander Kanitz}
\author[14]{Andreas Wilm}
\author[1,13]{Jan Forster}
\author[10,8]{Manuel Holtgrewe}
\author[11,16]{Christopher H. Tomkins-Tinch}
\author[12]{Sven Nahnsen}
\author[1,7,\corresponding]{Johannes Köster}

\affil[1]{Algorithms for reproducible bioinformatics, Genome Informatics, Institute of Human Genetics, University Hospital Essen, University of Duisburg-Essen, Essen, Germany}
\affil[2]{Institute of Pathology, University Hospital Essen, University of Duisburg-Essen, Essen, Germany}
\affil[3]{Biomedical Informatics, Harvard Medical School, Harvard University, Boston, USA}
\affil[4]{EMBL-EBI, Hinxton, UK}
\affil[5]{Stanford University Research Computing Center, Stanford University}
\affil[6]{Department of Biosystems Science and Engineering, ETH Zurich, Basel, Switzerland}
\affil[7]{Medical Oncology, Harvard Medical School, Harvard University, Boston, USA}
\affil[8]{Charité - Universitätsmedizin Berlin, corporate member of Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health (BIH), Center for Digital Health, Berlin, Germany}
\affil[9]{Biozentrum, University of Basel, Switzerland \& SIB Swiss Institute of Bioinformatics / ELIXIR Switzerland, Lausanne, Switzerland}
\affil[10]{CUBI – Core Unit Bioinformatics, Berlin Institute of Health, Berlin, Germany}
\affil[11]{Broad Institute of MIT and Harvard, Cambridge, MA, 02142, USA}
\affil[12]{TODO}
\affil[13]{TODO}
\affil[14]{TODO}
\affil[15]{Swiss Institute of Bioinformatics (SIB), Basel, Switzerland}
\affil[16]{Department of Organismic and Evolutionary Biology, Harvard University, Cambridge, MA, 02138, USA}
\sharedfirsttext{Shared first author}
\correspondingtext{To whom correspondence should be addressed}

\title{Sustainable data analysis with Snakemake}
\maketitle

\begin{abstract}
	In this paper we analyze the properties needed for data analysis to be reproducible, adaptable and transparent, and show how the popular workflow management system, Snakemake, can be used to fulfill all of these needs.
	Data analysis often requires a multitude of heterogeneous processes, from various command line tools for core analysis to scripting languages such as R or Python for cleaning data, creating tables, and generating figures. 
    It is widely recognized that such processes are best linked in a reproducible and replicable way; this enables technical validation of an analytical method and regeneration of results from the original, or even new, data.
	Reproducibility alone does not guarantee analysis will have with lasting (i.e. sustainable) impact for the field, or even just one research group: we postulate that it is equally important to ensure \emph{adaptability} and \emph{transparency}. An adaptable analysis is one that can be easily modified to answer extended or slightly different research questions. This is enabled by the transparency to understand whether an analysis is methodologically sound and technically valid.
	Workflow management systems such as Snakemake allow researchers to codify their computational analyses in a way that allows them to be reused, understood, and extended by others in their field. 
\end{abstract}

Despite the ubiquity of data analysis across scientific disciplines, it is a challenge to ensure \emph{in silico} reproducibility~\parencite{Mesirov2010,Baker2016,Munaf__2017}.

Consequently, a \emph{Cambrian explosion} of diverse scientific workflow management systems is in process; some are already in use by many and evolving, and countless others are emerging and being published (see~\url{https://github.com/pditommaso/awesome-pipeline}).

Existing workflow systems can be partitioned into five niches which we will describe below, with highlighted examples of each.

First, workflow management systems like Galaxy~\parencite{Afgan2018}---often referred to as ``science gateways``~\parencite{Afgan2011-tv}---offer graphical user interfaces for composition and execution of workflows. Such systems allow one to become proficient quickly without the need to write code, and their immediate web access makes them accessible to everyone with an internet connection.

Second, systems like Anduril~\parencite{Cervera2019}, Balsam~\parencite{papka2018}, Hyperloom~\parencite{cima2018hyperloom}, Jug~\parencite{Coelho_2017}, Pwrake~\parencite{Tanaka_2010}, Ruffus~\parencite{Goodstadt2010}, SciPipe~\parencite{Lampa2019}, SCOOP \parencite{SCOOP_XSEDE2014}, and COMPSs~\parencite{Lordan_2013} specify workflows using a set of generic classes and functions for programming languages such as Python, Scala, and others.
Such systems have the advantage that workflows are encoded in familiar languages, and described in text that can be straightforwardly managed with version control systems like Git (\url{https://git-scm.com}). They can be used without without a graphical interface, granting flexibility in the platforms on which they are run (e.g. from individual machines, or in a server environment).

Third, in systems like Nextflow~\parencite{Di_Tommaso_2017}, Snakemake~\parencite{Köster2012}, BioQueue~\parencite{Yao2017}, Bpipe~\parencite{Sadedin2012}, ClusterFlow~\parencite{Ewels2016}, Cylc~\parencite{J_Oliver_2018},~and BigDataScript~\parencite{Cingolani_2014}, workflows are specified using a domain-specific language (DSL).
Members of this niche share the advantages of the second niche, while adding the additional benefit of improved readability; the DSL provides statements and declarations that specifically model central components of workflow management, reducing the need for superfluous operators and boilerplate code.
For Nextflow and Snakemake, since the DSL is implemented as extensions to generic programming languages (Groovy and Python, respectively), these systems grant access to the full power of the underlying programming language (e.g. for implementing conditional execution and dynamic configuration).

Fourth, systems like Popper~\parencite{Jimenez_2017}, specify workflows in a purely declarative way, via configuration file formats like YAML~\parencite{Evans2009-vp}.
These declarative systems share the concision and clarity of the third niche, with workflow specifications that can be particularly readable for non-developers. The caveat of these benefits is that by disallowing imperative or functional programming, these workflow systems can be more restrictive in the processes that can expressed.

Fifth, there are execution system-independent workflow specification languages like CWL~\parencite{cwl} and WDL~\parencite{voss_full-stack_2017}.
These languages define a (declarative) syntax for specifying workflows, which decouples workflow \emph{description} from workflow \emph{execution}. Workflows written in these languages can be parsed and run by any executor adhering to the language specification; e.g. Cromwell (\url{https://cromwell.readthedocs.io}), Toil~\parencite{Vivian_2017}, and Tibanna~\parencite{Lee_2019}. Workflows created with these languages have the advantage of being platform-agnostic; they can be run on various back-ends, with executors enabling scalability and portability to heterogeneous computing environments.

At present, several members of the above niches allow for complete \emph{in silico} reproducibility of data analyses (e.g. Galaxy, Nextflow, Snakemake, WDL, CWL), by allowing, at each step, definition, deployment of software dependencies (e.g. via the Conda package manager,~\url{https://docs.conda.io}, Docker,~\url{https://www.docker.com}), or Singularity~\parencite{kurtzer_singularity_2017} containers), and scalable execution.

Reproducibility is important to generate trust in scientific results.
However, we postulate that a data analysis is of lasting value (and thus sustainability) for the authors and their scientific field only if a hierarchy of additional interdependent properties is ensured (see Fig.~\ref{fig:sustainability}).

\begin{figure}
	\image{sustainability-in-wms.pdf}
	\caption{
		Hierarchy of aspects to consider for sustainable data analysis.
		By supporting the top layer, a workflow management system can promote the center layer, and thereby help to obtain true sustainability.
	}\label{fig:sustainability}
\end{figure}

A data analysis gains full \emph{in silico} \emph{reproducibility} if it is \emph{automated}, \emph{portable}, and \emph{scalable}: 1) it must be able to be run with all software dependencies in exactly the needed versions, without manual configuration 2) it must be able to be automatically deployed to various computational platforms, and 3) it must run at varying levels of parallelism to suit project goals, the size of the datasets involved, and available hardware.

While being able to reproduce results is a major achievement, \emph{transparency} is equally important: the validity of results can only be fully assessed if the parameters, software, and custom code of each analysis step are fully accessible.
On the level of code, a data analysis therefore must be \emph{readable} and \emph{well-documented}.
On the level of the results, it must be possible to \emph{trace} parameters, code, and components of the software stack through all involved steps.

Finally, valid results yielded by a reproducible data analysis have greater meaning to the scientific community if the analysis to produce them can be reused for other projects.
In practice, direct reuse is almost never possible. Instead, workflows must be ~\emph{adaptable} to new circumstances; it must be possible to extend an analysis by replacing or modifying steps and adjusting parameters.
Once changes have been made, a workflow is only truly adaptable if it can be run: it must be possible to easily execute an analysis in the varying computational environments of those who have made changes (e.g. at different institutes, or by those with cloud compute in mind). For this, an analysis must again be \emph{portable} and \emph{scalable}.
In addition, analysis code must be as \emph{readable} as possible so that it can be easily modified by new adopters.

In this work, we show how Snakemake supports full \emph{in silico} \emph{reproducibility} in each of the aspects mentioned.

Since its original publication in 2012, Snakemake has seen hundreds of releases and contributions (\autoref{fig:citations}c).
It has gained wide adoption in the scientific community, culminating in, on average, more than six new citations per week, and over 700 citations in total (Fig.~\ref{fig:citations}a,b). This makes Snakemake one of the most widely-used workflow management systems in science.

\begin{figure}
	\image{citations+development.pdf}
	\caption{
		Citations and development of Snakemake.
		(a) citations by year of the original Snakemake article (note that the year 2020 is still incomplete at the time of writing).
		(b) citations by scientific discipline of the citing article.
		Data source:~\url{https://badge.dimensions.ai/details/id/pub.1018944052}, 2020/09/29.
		(c) cumulative number of git commits over time; Releases are marked as circles.
	}
	\label{fig:citations}
\end{figure}

\section{Results}

Here we present how Snakemake allows data analyses to meet all criteria of being reproducible, adaptable, and transparent, and how analyses created with Snakemake can create sustained value for their authors themselves and the broader scientific community.

We will introduce features of both the workflow definition language as well as the execution environment.
Several of these features are shared with other tools (e.g., Conda and container integration), while others are (at the time of writing) exclusive to Snakemake (e.g., the scheduling mechanism, the graph partitioning, the caching mechanism).

Finally, we address shortcomings by describing features of other tools which are (not yet) offered by Snakemake.

We explicitly refrain from performing a full comparison with other tools, as we believe such a summary will never be unbiased or lasting. Readers wishing to select a workflow system should instead consult current documentation, review recent articles, and personally evaluate and try each system for their individual needs.

\subsection{Automation}\label{sec:automation}

The central idea of Snakemake is that workflows can be specified through decomposition into steps represented as~\emph{rules~}(Fig.~\ref{fig:example}).

Each rule describes how to obtain a set of output files from a set of input files.

This can happen via a shell command, a block of Python code, an external script (Python, R, or Julia), a Jupyter notebook (\url{https://jupyter.org}), or a so-called wrapper (see Sec.~\ref{sec:modularization}).

Depending on the computing platform used and how Snakemake is configured, input and output files are either stored on disk, or in remote storage (e.g. FTP, Amazon S3, Google Storage, NCBI GenBank, etc.).

Through the use of wildcards, rules can be generic. For example, see the rule~\lstinline!select_by_country! in Fig.~\ref{fig:example}a (line 20): it can be applied to generate any output file of the form~\lstinline!results/by-country/{country}.csv!, with~\lstinline!{country}! being a wildcard that can be replaced with any non-empty string.

In shell commands, input and output files, as well additional parameters, are directly accessible by enclosing the respective keywords in curly braces (~\lstinline!{}!) (when a keyword represents more than one item, as in the case of a list or dictionary, access can happen by name or index).

\begin{figure}
	\image{example-workflow.pdf}
	\caption{
		Example Snakemake workflow. (a) workflow definition; hypothesized knowledge requirement for line readability is color-coded on the left next to the line numbers. (b) directed acyclic graph (DAG) of jobs, representing the automatically derived execution plan from the example workflow; job node colors reflect rule colors in the workflow definition. (c) content of script plot-hist.py referred from rule plot\_histogram. (d) knowledge requirements for readability by statement category.
	}
	\label{fig:example}
\end{figure}

When using script integration instead of shell commands, Snakemake automatically inserts an object giving access to all properties of the job (e.g. \lstinline!snakemake.output[0]!, see Fig.
\ref{fig:example}c).
This avoid the presence and repetition of boiler plate code for parsing command line arguments.

Following replacement of wildcards with concrete values, Snakemake turns any rule into a job which can be executed to generate the defined output files.

Dependencies between jobs are implicit, and inferred automatically in the following way: for each input file of a job, Snakemake determines a rule that can generate it---for example by replacing wildcards again (ambiguity can be resolved by prioritization or constraining wildcards)---yielding another job.
Then, Snakemake goes on recursively upward from ultimate output to initial inputs, until all input files of all jobs are either generated by another job or already present in the used storage (e.g. on disk).
Where necessary, it is possible to provide arbitrary Python code to infer input files based on wildcard values or the contents of output files generated by upstream jobs.

\autoref{fig:example}a illustrates all major design patterns needed to define workflows with Snakemake: workflow configuration (line 1), aggregations (line 5-8), specific (line 33-43) and generic (line 45-53) transformations, target rules (line 3-8), log file definition, software stack definition, as well as shell command, script, and wrapper integration.
Supplementary \autoref{s-sec:design-patterns} presents additional, more exotic patterns that can nevertheless be helpful in certain situations (e.g. conditional execution).

\subsection{Readability}

The workflow definition language of Snakemake is designed to allow maximum readability, which is crucial for transparency and adaptability.
For natural-language readability, the occurrence of known words is important.~For example, the Dale-Chall readability formula derives a score from the fraction of potentially unknown words (that do not occur in a list of common words) among all words in a text~\parencite{chall_readability_1995}.
For workflow definition languages, one has to additionally consider whether punctuation and operator usage is intuitively understandable.
When analyzing the above example workflow (Fig.~\ref{fig:example}a) under these aspects, code statements fall into seven categories (supplementary \autoref{s-sec:readability}).
In addition, for each line, we can judge whether it needs:

\begin{enumerate}
	\item domain knowledge (from the field analyzed in the given workflow),
	\item technical knowledge (e.g. about Unix-style shell commands or Python),
	\item Snakemake knowledge,
	\item general education (e.g. it should be understandable for everybody).
\end{enumerate}

In Fig.~{\ref{fig:example}}, we hypothesize the required knowledge for readability of each code line.
Most statements are understandable with either general education, domain, or technical knowledge.
In particular, only five lines need Snakemake-specific knowledge (Fig.~{\ref{fig:example}}d).
The rationale for each hypothesis can be found in supplementary \autoref{s-sec:readability}.

While this example is obviously not as evolved as a real-world data analysis, this example parallels real-world uses in the the ratio of the number of lines requiring Snakemake knowledge to the number of lines which are readable with general education, domain, or technical knowledge.

Since Snakemake supports modularization of workflow definitions, it is possible to hide technical parts of the workflow definition (e.g. helper functions or variables), in order to not distract the reader from understanding the main steps of the data analysis.

Since dependencies between jobs are implicitly encoded via matching filename patterns, we hypothesize that, in general, no specific technical knowledge is necessary to understand the connections between the rules. One exception to this exists when conditional are used or dependencies are determined dynamically.
The file-centric description of workflows makes it intuitive to to infer dependencies between steps; when the input of one rule reoccurs as the output of another, their link and order of execution is clear.

\subsubsection{Modularization}\label{sec:modularization}

Specific data analysis steps can become quite complicated, e.g., when plotting figures or working around idiosyncrasies of external tools.
It helps readability to modularize longer processes like these so the reader of a workflow can inspect a smaller component if they are interested in the specific step.
Some of these steps can be quite specific and unique to an analysis.
Others can be common to the scientific field and utilize popular tools or libraries in a relatively standard way.
For the latter, Snakemake provides the ability to deposit and use \emph{tool wrappers} in/from a central repository.
In contrast, the former can require small blocks of custom code, often written in various scripting languages like R or Python.

Snakemake allows the user to further modularize such steps either into scripts or as interactive Jupyter notebooks (\url{https://jupyter.org}).

\paragraph{Script integration.}
Integrating a script works via a special \lstinline!script! directive (see \autoref{fig:example}a, line~42).
The referred script does not need any boilerplate code, and can instead directly use all properties of the job (input files, output files, wildcard values, parameters, etc.), which are automatically inserted as a global \lstinline!snakemake! object before the script is executed (see \autoref{fig:example}c).

\paragraph{Jupyter notebook integration.}
Analogous to script integration, a \lstinline!notebook! directive allows a rule to specify a path to a Jupyter notebook. An option of the command line instructs Snakemake to open a Jupyter notebook server for editing a notebook in the context of a specific job derived from the rule that refers to the notebook.
The notebook server can be accessed via a web browser in order to interactively program the notebook until the desired results (e.g. a certain plot or figure) are created as intended.
Upon saving the notebook, Snakemake generalizes it such that other jobs from the same rule can subsequently re-use it automatically without the need for another interactive notebook session.

\paragraph{Tool wrappers.}
Reoccurring tools or libraries can be shared between workflows via Snakemake tool wrappers.
A central public repository (\url{https://snakemake-wrappers.readthedocs.io}) allows the community to share wrappers with each other.
Each wrapper consists of a Python or R script that either uses libraries of the respective scripting language, or a script that calls a shell command.
Moreover, each wrapper provides a Conda environment defining the software stack required, including tool and library versions (see \autoref{sec:portability}).
Often, shell command wrappers contain some additional code that works around various idiosyncrasies of the wrapped tool (e.g. dealing with temporary directories or converting job properties into command line arguments).

A wrapper can be used in a rule by simply copying and adapting a provided example rule (e.g. by modifying input and output file paths).

Upon execution, the wrapper code and the Conda environment description are downloaded from the repository and automatically deployed to the running system.

All wrappers are automatically tested to run without errors prior to inclusion in the repository, and upon each committed change. In addition to single wrappers, the wrapper repository also offers pre-defined, tested combinations of wrappers that constitute entire sub-workflows for common tasks (called meta-wrappers). 
This is particularly useful for combinations of steps that reoccur in many data analyses.

\subsubsection{Standardized code linting and formatting.}\label{sec:style}

The readability of programming code can be heavily influenced by adhering to common style and best practices \parencite{tysell_sundkvist_code_2017}.
Snakemake provides automatic code formatting (via the tool \lstinline!snakefmt!) of workflows, together with any contained Python code.
In addition, Snakemake has a built in \emph{code linter} that detects code violating best practices (e.g. missing directives, indentation issues, missing environment variables, etc.) and provides suggestions on how to improve the code.

\subsection{Portability}\label{sec:portability}

Being able to deploy a data analysis workflow to an unprepared system depends on: (a) the ability to install the workflow management system itself, and (b) the ability to obtain and use the required software stack for each analysis step.

Snakemake itself is easily deployable via the Conda package manager (\url{https://conda.io}), as a Python package (\url{https://pypi.io}) or a Docker container (\url{https://hub.docker.com/r/snakemake/snakemake}).

The management of software stacks needed for individual rules is directly integrated into Snakemake itself, via two complementary mechanisms.

\paragraph{Conda integration}

For each rule, it is possible to define a software environment that will be automatically deployed via the Conda package manager (via a \lstinline!conda! directive, see \autoref{fig:example}a, line 15). Each environment is described by a lightweight YAML file used by conda to install constituent software.
While efficiently sharing base libraries like Glib with the underlying operating system, software defined in the environment takes precedence over the same software in the operating system, and is isolated and independent from the same software in other Conda environments. 

\paragraph{Container integration}

Instead of defining Conda environments, it is also possible to define a container for each rule (via a \lstinline!container! directive, see \autoref{fig:example}a, line 38).
Upon execution, Snakemake will pull the requested container image and run a job inside that container using Singularity \parencite{kurtzer_singularity_2017}.
The advantage of using containers is that the execution environment can be controlled down to the system libraries, and becomes portable across operating systems, thereby further increasing reproducibility \parencite{gruning_practical_2018}. Containers already exist in centralized repositories for a wide range of scientific software applications, allowing easy integration info Snakemake workflows. Downsides of using containers are that they require storage on disk and additional effort is required to create them. A sweet spot between containers and Conda can be exploited by combining \lstinline!container! and \lstinline!conda! directives.
In that case, Snakemake will generate the requested Conda environment inside of the container, providing the flexibility of Conda together with the additional reproducibility guarantees of containers.

\subsection{Traceability and documentation}

While processing a workflow, Snakemake tracks input files, output files, parameters, software, and code of each executed job.
After completion, this information can be made available via self-contained, interactive, static HTML reports.
Output files in the workflow can be annotated for automatic inclusion in the report.
These features enable interactive exploration of results alongside information about their provenance.
Since results are embedded in such reports, their presentation does not depend on interactive server backends; this makes Snakemake reports portable and achievable.
In the future, Snakemake reports will be extended to follow the RO-crate standard, which will make them machine-readable and allow integration with web services like \url{https://workflowhub.eu} to allow for search indexing and in-line display.

An example report is included as supplementary file 1.

\subsection{Scalability}\label{sec:scalability}

Being able to scale a workflow to available computational resources is crucial for reproducing previous results as well as for adapting a data analysis for novel research questions or datasets.

Like many other state-of-the-art workflow management systems, Snakemake allows workflow execution to scale to various computational platforms, ranging from single workstations to large compute servers, any common cluster middleware (e.g. via DRMAA), grid computing, and cloud computing (with native support for Kubernetes, the Google Life Sciences API, Amazon AWS, TES (\url{https://www.ga4gh.org}), and Microsoft Azure, the latter two in an upcoming release).

Snakemake's design ensures that only modification of command line parameters at the start of execution is required to scale a workflow to a specific compute platform; the workflow itself remains untouched.
Via configuration profiles, it is possible to persist and share the command line setup of Snakemake for any computing platform (\url{https://github.com/snakemake-profiles/doc}).

\subsubsection{Job scheduling}\label{sec:scheduling}

Naturally, due to their dependencies, not all jobs in a workflow can be executed at the same time.
Instead, one can imagine partitioning the DAG of jobs into three sections---those that are already finished, those that have already been scheduled but are not finished yet, and those that have not yet been scheduled (\autoref{fig:scheduling}a).
Within the latter partition, all jobs that have no incoming edge from the same partition can be scheduled next.
We call this the set $J$ of pending jobs.
The scheduling problem a workflow manager like Snakemake has to solve is to select the subset $E \subseteq J$ that leads to an optimal execution of the workflow, while not exceeding given computational resources (see \autoref{fig:scheduling}b,c for example solutions).
Optimality of execution is thereby fueled by three criteria:
First, it should be as fast as possible.
Second, high-priority jobs should be preferred (Snakemake allows prioritization of jobs via the workflow definition and the command line).
Third, temporary output files should be deleted as fast as possible (Snakemake allows output files to be marked as temporary, which leads to their automatic deletion once all consuming jobs have been finished).
The latter guarantees the second, optimal, example scheduling solution (\autoref{fig:scheduling}c).

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{scheduling.pdf}
	\caption{Snakemake scheduling problem. (a) Example workflow DAG.
		The greenish area depicts those jobs that are ready for scheduling (because all input files are present) at a given time during the workflow execution.
		In addition, we assume that the red job at the root generates a temporary file, which could be deleted once all blue jobs are finished.
		(b) Suboptimal scheduling solution: two green jobs are scheduled, such that only one blue job can be scheduled and the temporary file generated by the red job has to remain on disk until all blue jobs are finished in a subsequent scheduling step. (c) Optimal scheduling solution: the three blue jobs are scheduled, such that the temporary file generated by the red job can be deleted afterwards.}\label{fig:scheduling}
\end{figure}

\newcommand{\N}{\mathbb{N}}
\newcommand{\cores}{\text{c}}

We solve the scheduling problem via linear programming as follows.
Let $R$ be the set of resources used in the workflow (e.g., CPU cores and memory; by default, Snakemake only considers CPU cores which we in the following indicate with $\cores$, i.e., $R = \{\cores\}$).
Let $F$ be the set of temporary files that will be generated during the workflow.
We first define constants for each job $j \in J$: let $p_j \in \N$ be its priority, let $u_{r,j} \in \N$ be its usage of resource $r \in R$, and let $z_{f,j} \in \{0,1\}$ indicate whether it consumes temporary file $f \in F$ ($1$) or not ($0$).
Further, let $U_r$ be the free capacity of resource $r \in R$ (initially what is provided to Snakemake on the command line, later what is left given already running resources), and let $S_f$ be the size of file $f \in F$.

Next, we define variables $x_j \in \{0,1\}$ for each job $j \in J$, indicating whether a job is selected for execution ($1$) or not ($0$).
For each temporary file $f \in F$, we define a variable $\delta_f \in [0,1]$ indicating the fraction of consuming jobs that have already or will be scheduled.
We also call this variable the lifetime fraction of temporary file $f$.
In other words, $\delta_f = 1$ means that all consuming jobs will be completed after this scheduling round has been processed, in other words, the lifetime of that file is over and it can be deleted.
To indicate the latter, we further define a binary variable $\gamma_f \in \{0,1\}$, with $\gamma_f = 1$ representing the case that $f$ can indeed be deleted, in other words, $\gamma_f = 1 \Leftrightarrow \delta_f = 1$.

Then, the scheduling problem can be written as a linear optimization problem:
\begin{align*}
	\text{maximize } & 2U_\cores \cdot 2S \cdot \sum_{j \in J} x_j \cdot p_j + 2S \cdot \sum_{j \in J} x_j \cdot (u_{\cores,j} + 1) + S \cdot \sum_{f \in F} \gamma_f \cdot S_f + \sum_{f \in F} \delta_f \cdot S_f \\ \text{subject to } & x_j \in \{0,1\} \quad \forall j \in J\\ \quad & \delta_f \in [0,1] \quad  \forall f \in F\\ \quad & \gamma_f \in \{0,1\} \quad \forall f \in F\\ \quad & \sum_{j \in J} x_j \cdot u_{r,j} \leq U_r \quad \forall r \in R\\ \quad & \delta_f \leq \frac{\sum_{j \in J} x_j \cdot z_{f,j}}{\sum_{j \in J} z_{f,j}} \quad\forall f \in F\\ \quad & \gamma_f \leq \delta_f \quad\forall f \in F.
\end{align*}

The first constraint ensures that the available amount $U_r$ of each resource $r \in R$ is not exceeded by the selection.
The second constraint (together with the fact that it occurs in the maximization) calculates the lifetime fraction of each temporary file $f \in F$.
The third constraint (together with the fact that it occurs in the maximization) calculates whether temporary file $f \in F$ can be deleted.

The maximization optimizes four criteria, represented by the four terms that are summed up.
First, we strive to prefer jobs with high priority.
Second, we aim to maximize parallelization or the number of used cores\footnote{Snakemake also allows for jobs that use zero cores (i.e. $u_{\cores,j} = 0$), hence we add $+1$ to each summand of the second term.}.

Third, we ensure that temporary files are deleted as soon as possible.
Fourth, we try to reduce the lifetime of temporary files that cannot be deleted in this pass.

The four criteria shall be considered in lexicographical order, in other words: priority is most important, only upon ties do we consider parallelization.
Given ties while optimizing parallelization, we consider the ability to delete temporary files.
And only given ties when considering the latter, we take the lifetime of all temporary files that cannot be deleted immediately into account.
Technically, this order is enforced by multiplying each criterion sum with a value that is at least as high as the maximum value that the equation right of it can acquire.

Unless the user explicitly requests it, all jobs will have the same priority, meaning that in general the optimization problem maximizes the number of used cores while trying to get rid of as many temporary files as possible.

\subsubsection{Caching between workflows}

While data analyses usually entail the handling of multiple datasets or samples that are specific to a particular project, they often also rely on retrieval and post-processing of common datasets.
For example, in the life sciences, such datasets include reference genomes and corresponding annotations.
Since such datasets potentially reoccur in many analyses conducted in a lab or at an institute, re-executing the  analysis steps for retrieval and post-processing of common datasets as part of individual analyses would waste both disk space and computation time.

Historically, the solution in practice was to compile shared resources with post-processed datasets that could be referred to from the workflow definition.
For example, in the life sciences, this has led to the Illumina iGenomes resource (\url{https://support.illumina.com/sequencing/sequencing\_software/igenome.html}), and the GATK resource bundle (\url{https://gatk.broadinstitute.org/hc/en-us/articles/360035890811-Resource-bundle}).
In addition, in order to provide a more flexible way of selection and retrieval for such shared resources, so-called "reference management" systems have been published, like Go Get Data (\url{https://gogetdata.github.io}) and RefGenie (\url{http://refgenie.databio.org}).
Here, the logic for retrieval and post-processing is curated in a set of recipes or scripts, and the resulting resources can be automatically retrieved via command line utilities.
The downside of all these approaches is that the transparency of the data analysis is hampered by the steps taken to obtain the data, and the processes required are hidden from or inaccessible to the reader of the data analysis.

Snakemake provides a new, generic approach to the problem which does not have this downside (see \autoref{fig:caching}).
Leveraging workflow-inherent information, Snakemake can calculate a hash value for each job that unambiguously captures exactly how an output file is generated, prior to actually generating the file.
This hash can be used to store and lookup output files in a central cache (e.g. a folder on the same machine or in a remote storage).
For any output file in a workflow, if the corresponding rule is marked as eligible for caching, Snakemake can quickly copy or link to the file in the cache if it has been created before, even in a different workflow or by a different user on the same system, thereby saving computation time as well as disk space.

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{caching.pdf}
	\caption{
		Blockchain based between workflow caching scheme of Snakemake.
		If a job is eligible for caching, its code, parameters, raw input files, software environment and the hashes of its dependencies are used to calculate a SHA-256 hash value, under which the output files are stored in a central cache.
		Subsequent runs of the same job (with the same dependencies) in other workflows can skip the execution and directly take the output files from the cache.
	}
	\label{fig:caching}
\end{figure}

The hash value is calculated in the following way.
Let $J$ be the set of jobs of a workflow.
For any job $j \in J$, let $c_j$ denote its code (shell command, script, wrapper, or notebook), let $P_j = \{(k_i, v_i) \mid i=0,\dots,m\}$ be its set of parameters (with key $k_i$ and JSON-encoded value $v_i$), let $F_j$ be its set of input files that are not created by any other job, and let $s_j$ be a string describing the associated software environment (either a container unique resource identifier, a Conda environment definition, or both).
Then, assuming that job $j \in J$ with dependencies $D_j \subset J$ is the job of interest, we can calculate the hash value as $$ h(j) = h'\left( c_j \oplus \left(\bigoplus_{i=0}^m k_i \oplus v_i \right) \oplus \left( \bigoplus_{f \in F_j} h'(f) \right) \oplus s_j \oplus \left( \bigoplus_{j' \in D_j} h(j') \right) \right) $$ with $h'$ being the SHA-256 \parencite{Handschuh} hash function, $\oplus$ being the string concatenation, and $\bigoplus$ being the string concatenation of its operands in lexicographic order.

The hash function $h(j)$ comprehensively describes everything that affects the content of the output files of job~\(j\), namely code, parameters, raw input files, the software environment and the input generated by jobs it depends on.
For the latter, we recursively apply the hash function~\(h\) again.
In other words, for each dependency~\(j' \in D_j\) we include a hash value into the hash of job~\(j\), which is in fact the hashing principle behind blockchains used for cryptocurrency \parencite{narayanan_bitcoin_2016}.

\subsubsection{Graph partitioning}\label{sec:partitioning}

A data analysis workflow can contain diverse compute jobs, some of which may be long-running, and some which may complete quickly. When executing a Snakemake workflow in a cluster or cloud setting, by default, every job will be submitted separately to the underlying queuing system.
For short-running jobs, this can result in a considerable overhead, as jobs wait in a queue, and may also incur additional delays or cost when accessing files from remote storage or network file systems.
To minimize such overhead, Snakemake offers the ability to partition the DAG of jobs into subgraphs that will be submitted together, as a single cluster or cloud job.

Partitioning happens by assigning rules to groups (see \autoref{fig:grouping}).
Upon execution, Snakemake determines connected subgraphs with the same assigned group for each job, and submits such subgraphs together (as a so called \emph{group job}) instead of submitting each job separately.
For each group, it is  possible to define how many connected subgraphs will be spanned when submitting (one, by default).
This way, it is possible to adjust the partition size to the needs of the available computational platform.
The resource usage of a group job is determined by sorting involved jobs topologically, summing resource usage per-level and taking the maximum over all levels.

\begin{figure}
	\image{group-jobs.pdf}
	\caption{Job graph partitioning by assigning rules to groups.
		Two rules of the example workflow (\autoref{fig:example}a) are grouped together, (a) spanning one connected component, (b) spanning two connected components, and (c) spanning five connected components.
		Resulting submitted group jobs are represented as grey boxes.
	}\label{fig:grouping}
\end{figure}

\subsubsection{Streaming}\label{sec:streaming}

Sometimes, intermediate results of a data analysis can be huge, but not important enough to store persistently on disk.
Apart from the option to mark such files as temporary so that Snakemake will automatically delete them when no longer needed, it is also possible to instruct Snakemake to never store them on disk at all by directly streaming their content from the producing job to to the consuming job.
This requires the producing and consuming jobs have run at the same time on the same computing node.
Snakemake ensures this by submitting producer and consumer as a group job (see \autoref{sec:partitioning}).

\section{Conclusion}

While reproducibility has a goal of high importance for data analysis workflow management in recent years, reproducibility alone does not motivate scientists, engineers, and technicians when crafting data analyses.

Here, we have outlined benefits enabling and beyond reproducibility: automation, scalability, portability, readability, traceability, and documentation, and how these factors help make data analyses adaptable and transparent. These qualities allow data analyses to be created not solely as fulcrums for one person or one lab, but as force multipliers for the broader community in which they work.

Adaptable data analyses can not only be repeated on the same data, but also be modified and extended for new questions or scenarios, greatly increasing their value for both the scientific community and the original authors.
While reproducibility is a necessary property for checking the validity of scientific results, it is helpful to go farther; the portability of being able to reproduce exactly the same figure on an entirely different machine tells us that an analysis is also robust and valid from a technical perspective. Methodological validity (correctness of statistical assumptions, avoidance of overfitting, etc.) can only be assessed through transparent and accessible analysis code.

By analyzing its readability and presenting its modularization, portability, reporting, scheduling, caching, partitioning, and streaming abilities, we have shown how Snakemake supports all of these aspects, and provides a comprehensive framework for sustainable data analysis.

\section{Author contributions}
Felix Mölder has designed and implemented the job scheduling mechanism (\autoref{sec:scheduling}).
Kim Philip Jablonski has designed and implemented Jupyter notebook integration (\autoref{sec:modularization}).
Michael Hall and Brice Letcher have designed and implemented automated code formatting (\autoref{sec:style}).
Vanessa Sochat has designed and implemented the Google Life Sciences API execution backend, as well as various improvements to Google storage support (\autoref{sec:scalability}).
Soohyun Lee has designed and implemented the AWS execution backend via integration with Tibanna (\autoref{sec:scalability}).
Sven O.
Twardziok and Alexander Kanitz have designed and implemented the TES execution backend (\autoref{sec:scalability}).
Andreas Wilm has designed and implemented the Microsoft Azure execution backend (\autoref{sec:scalability}).
Manuel Holtgrewe has designed and implemented benchmarking support (supplementary \autoref{s-sec:design-patterns}).
Jan Forster has designed and implemented meta-wrapper support (\autoref{sec:modularization}).
Christopher Tomkins-Tinch has designed and implemented remote storage support and edited the manuscript (\autoref{sec:automation}).
Christian Meesters has supervised the implementation of an initial prototype of the graph partitioning functionality (\autoref{sec:partitioning}).
Sven Nahnsen has provided the initial idea of using blockchain hashing to fingerprint output files a priori.
Johannes Köster has written the manuscript and implemented all other features that occur in the text but are not explicitly mentioned in above listing.

\section{Acknowledgements}
We are most grateful for the thousands of Snakemake users, their enhancement proposals, bug reports, and efforts to perform sustainable data analyses.
We deeply thank all contributors to the Snakemake, Snakemake-Profile, Snakemake-Workflows, and Snakemake-Wrappers codebases.

\printbibliography
\end{document}
